/*! \page running_in_mpi Running OSPRay in MPI Mode 


  OSPRay supports various different modes of running MPI-parallel.
  Here's how to run them:

  \section one_mpi_ring One MPI group; app on rank 0, ospray on ranks 1-N

  <code>
  mpirun -np N \<otherMPIOptions\> ./myApp --osp:mpi \<otherAppOptions\>
  </code>

  This mode is particularly intended for users wanting to run a
  single-process vis application in a HPC resource through VNC. We
  assume that both application and workers are running within the same
  MPI group / job allocation.

  In this mode we use mpirun to launch N application instances on
  whatever nodes are specified by the user. The command line flag
  "--osp:mpi" tells ospray that only rank 0 should run the
  application; all other ranks will run ospray workers. In addition to
  the application, rank 0 will also run an ospray coordinator that
  (dynamically) load-balances the workers. 

  By default this mode will use a local frame buffer on rank 0, and
  all workers will send pixel tiles to the coordinator. Ergo this mode
  assumes that rank 0 has reasonably good network connectivity to all
  rank nodes.

  \section launch_local_cluster Automatically Launching on a Local Cluster

  In this mode we pass the name of an MPI launch script that will
  launch the actual workers; the app-part of ospray will then
  automatically connect to these workers.

  To start the app, simply do
  <code>
  ./myApp --osp:mpi-launch <launchscript> <otherAppOptions>
  </code>

  A sample app script might look like this:
  <code>
  ibrun-mic2 -np 4 ./ospray_mpi_worker
  </code>

  The advantage of this method is that the user does not have to
  specify mpirun explicitly, and, in paritulcar, that since the launch
  script only runs the 'ospray_mpi_worker' it can also schedule
  workers onto nodes on which the app itself might not run (eg, the
  mics do not have opengl, and would not run an opengl app; the
  ospray_mpi_worker does not have this limitation).



\page Running an MPI-parallel OSPRay on Stampede - Step by Step instructions

Note for now all VNC Sessions on Stampede HAVE to run on the vis queue.
<ul>
<li>Create a VNC shell.
<code>
sbatch -A StampedeIntel -p vis -t 04:00:00  /share/doc/slurm/job.vnc -geometry 1600x1200
</code>
(Note: alternative queues would be 'normal-mic'

<li>Get VNC server info:
<code>
touch vncserver.out ; tail -f vncserver.out
</code>

<li>Connect to this VNC server using your vnc client (e.g., tigervnc on Mac)

<li>make sure to set the mpi fabric to dapl:
<code>export I_MPI_FABRICS=dapl:tcp</code>
<li>make sure you have intel and intel mpi loaded, and that you're using the right fabric.
<code>
module load intel
module swap mvapich2 impi/4.1.0.030
export I_MPI_FABRICS=dapl:tcp
</code>

<li>run your ospray-enabled app with "--osp:mpi-launch <launchscript>", where "launchscript" would be
<dl>
<dt>ospraydir/doc/launch-stampede-hosts.sh</dt><dd>launches the worker on all HOST nodes (ie, not on the MICs) allocated as part of the initial slurm allocation used to start the VNC serveradfadf</dd>
<dt>ospraydir/doc/launch-stampede-symm.sh</dt><dd>launches the worker on ALL nodes (INCLUDING the MICs) allocated as part of the initial slurm allocation used to start the VNC server</dd>
</dl>

</ul>
 */
