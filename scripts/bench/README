Running Benchmarks
------------------

1. Build OSPRay

Compile OSPRay in a build directory of your choosing. We will refer
to this directory as ${OSPRAY_BUILD_DIR}.

2. Fetch benchmark data using provided script

In ${OSPRAY_BUILD_DIR}, run the "get_benchmark_data.sh" script. Thus
from a "build" directory inside your cloned OSPRay source, you would
run:

% ../scripts/bench/get_benchmark_data.sh

3. Run benchmark script

In ${OSPRAY_BUILD_DIR}, run the "run_benchmark.py" script. Using the
same directory structure from step 2, you would run:

% ../scripts/bench/run_benchmark.py

The benchmarks will use a 1024x1024 frame buffer by default, which can
be modified by passing the width and height to the script. For example,
to run the benchmarks at 1024x768, you would run:

% ../scripts/bench/run_benchmark.py --width 1024 --height 768

The images rendered for each benchmark are saved into a directory (which
is created if it doesn't exist) "bench_output", where each image is saved
as a .ppm file. Statistics like average time it took to render a frame
are printed to standard output, but can also be saved to a file by
passing a filename to the script like this:

% ../scripts/bench/run_benchmark.py --output STATS.csv

By default, all test cases will be run. You can change it by passing a
comma-separated list of test names to the script, for example:

% ../scripts/bench/run_benchmark.py --tests NAME1,NAME2,NAME3

To print all valid names, run the script with --tests-list argument.

You can pass a path to directory with baseline images as an argument.
If you do, after running the test case, the script will compare
provided image with the rendered one, failing the test case if they
differ too much. For example:

% ../scripts/bench/run_benchmark.py --reference directory_with_images

Same goes for statistics - you can pass a path to a file that was
generated using --output option. Benchmark will fail if average times
spent on rendering single frame differ greatly. For example:

% ../scripts/bench/run_benchmark.py --baseline file_with_stats

Finally, you can chose between running tests using scivis renderer,
pathtracer, or both of them:

% ../scripts/bench/run_benchmark.py --renderer RENDERER

where RENDERER is either "both", "scivis" or "pt". The default
value is "both".

